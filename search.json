[
  {
    "objectID": "load.html",
    "href": "load.html",
    "title": "load",
    "section": "",
    "text": "source\n\nsmooth_func\n\n smooth_func (f_in:numpy.ndarray=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nf_in\nndarray\nNone\nUnsmoothed array\n\n\nReturns\nndarray\n\nSmoothed array\n\n\n\n\nsource\n\n\nload_boost_data\n\n load_boost_data (Bk_fileIn:str='/home/runner/work/CubicGalileonEmu/CubicG\n                  alileonEmu/CubicGalileonEmu/data/Boost.npy', Zk_fileIn:s\n                  tr='/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/\n                  CubicGalileonEmu/data/z_k.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nBk_fileIn\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/data/Boost.npy\nInput file for Boost\n\n\nZk_fileIn\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/data/z_k.txt\nInput file for redshift and wavenumbers\n\n\nReturns\ntuple\n\nBoost, Smoothed Boost, wavenumbers, redshifts\n\n\n\n\nsource\n\n\nload_boost_data_lin\n\n load_boost_data_lin (Bk_fileIn:str='/home/runner/work/CubicGalileonEmu/Cu\n                      bicGalileonEmu/CubicGalileonEmu/data/Boost_Lin.npy',\n                      Zk_fileIn:str='/home/runner/work/CubicGalileonEmu/Cu\n                      bicGalileonEmu/CubicGalileonEmu/data/z_k.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nBk_fileIn\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/data/Boost_Lin.npy\nInput file for Boost\n\n\nZk_fileIn\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/data/z_k.txt\nInput file for redshift and wavenumbers\n\n\nReturns\ntuple\n\nBoost, wavenumbers, redshifts\n\n\n\n\nsource\n\n\nload_params\n\n load_params (p_fileIn:str='/home/runner/work/CubicGalileonEmu/CubicGalile\n              onEmu/CubicGalileonEmu/data/cosmo_newdesign.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np_fileIn\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/data/cosmo_newdesign.txt\nInput file for parameters\n\n\nReturns\narray\n\nParameters\n\n\n\n\nsource\n\n\nsepia_data_format\n\n sepia_data_format (design:&lt;built-infunctionarray&gt;=None, y_vals:&lt;built-\n                    infunctionarray&gt;=None, y_ind:&lt;built-\n                    infunctionarray&gt;=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndesign\narray\nNone\nParams array of shape (num_simulation, num_params)\n\n\ny_vals\narray\nNone\nShape (num_simulation, num_y_values)\n\n\ny_ind\narray\nNone\nShape (num_y_values,)\n\n\nReturns\nSepiaData\n\nSepia data format",
    "crumbs": [
      "load"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CubicGalileonEmu",
    "section": "",
    "text": "Modified Gravity emulator for boost in the dark matter power spectra",
    "crumbs": [
      "CubicGalileonEmu"
    ]
  },
  {
    "objectID": "index.html#install-for-development-not-production",
    "href": "index.html#install-for-development-not-production",
    "title": "CubicGalileonEmu",
    "section": "Install (for development, not production)",
    "text": "Install (for development, not production)\ngit clone https://github.com/nesar/CubicGalileonEmu.git\ncd CubicGalileonEmu/\npip install -e '.[dev]'",
    "crumbs": [
      "CubicGalileonEmu"
    ]
  },
  {
    "objectID": "index.html#basic-rundown",
    "href": "index.html#basic-rundown",
    "title": "CubicGalileonEmu",
    "section": "Basic rundown",
    "text": "Basic rundown\n\nA few imports\n\nfrom CubicGalileonEmu.load import *\nfrom CubicGalileonEmu.viz import *\nfrom CubicGalileonEmu.pca import *\nfrom CubicGalileonEmu.gp import *\nfrom CubicGalileonEmu.emu import *\nfrom CubicGalileonEmu.mcmc import *\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n\n\nif_train_all = False ## Re-train all the models. Time-consuming. \nif_mcmc_all = False  ## Full MCMC run. Time-consuming. \nif_savefig = False\n\n\n\nLoading files\n\nBk_all, Bk_all_smooth, k_all, z_all = load_boost_data()\nBk_lin_all, _, _ = load_boost_data_lin()\np_all = load_params()\n\n\n\nA few plotting routines\n\nExperimental design\n\ndf_train_a = pd.DataFrame(p_all, columns=PARAM_NAME)\ncolors = ['b']*p_all.shape[0]\n# colors = ['b']*num_sims + ['r']*num_sims_test\nplot_scatter_matrix(df_train_a, colors);\n\n\n\n\n\n\n\n\n\n\nBoost metrics colored by cosmology parameters\n\ncolor_by_index = 4\nz_index = 0\n\nf, ax = plt.subplots(1, 2, figsize=(14, 4))\n\nplot_lines_with_param_color(p_all[:, color_by_index], \n                            k_all, \n                            Bk_all[:, z_index, :], \n                            'Training data, z=' + str(z_all[z_index]), \n                            r'$k [h/Mpc]$', \n                            r'$B(k)$', \n                            PARAM_NAME[color_by_index], ax=ax[0]);\n\nplot_lines_with_param_color(p_all[:, color_by_index], \n                            k_all, \n                            Bk_all[:, z_index, :]/Bk_lin_all[:, z_index, :], \n                            'Training data, z=' + str(z_all[z_index]), \n                            r'$k [h/Mpc]$', \n                            r'$Q(k)$', \n                            PARAM_NAME[color_by_index], ax=ax[1]);\n\n\n\n\n\n\n\n\n\ncolor_by_index = 3\nz_index = 21\n\nf, ax = plt.subplots(1, 2, figsize=(14, 4))\n\n\nplot_lines_with_param_color(p_all[:, color_by_index], \n                            k_all, \n                            Bk_all[:, z_index, :], \n                            'Training data, z=' + str(z_all[z_index]), \n                            r'$k [h/Mpc]$', \n                            r'$B(k)$', \n                            PARAM_NAME[color_by_index],\n                            ax=ax[0]);\n\nplot_lines_with_param_color(p_all[:, color_by_index], \n                            k_all, \n                            Bk_all[:, z_index, :]/Bk_lin_all[:, z_index, :], \n                            'Training data, z=' + str(z_all[z_index]), \n                            r'$k [h/Mpc]$', \n                            r'$Q(k)$', \n                            PARAM_NAME[color_by_index],\n                            ax=ax[1]);\n\n\n\n\n\n\n\n\n\nz_index = 0\n\nf, ax = plt.subplots(1, 2, figsize=(14, 4))\n\n\nf = plot_lines_with_param_color(z_all, \n                            k_all, \n                            Bk_all[16, :, :],\n                            'Training data', \n                            r'$k [h/Mpc]$', \n                            r'$B(k)$', \n                            'redshift',\n                            ax=ax[0]);\n\nf = plot_lines_with_param_color(z_all, \n                            k_all, \n                            Bk_all[16, :, :]/Bk_lin_all[16, :, :],\n                            'Training data', \n                            r'$k [h/Mpc]$', \n                            r'$Q(k)$', \n                            'redshift',\n                            ax=ax[1]);\n\n\n\n\n\n\n\n\n\n\n\nTraining involves: PCA, GP fitting.\n\n## Data prep\nz_index = 8\n# y_vals = Bk_all[:, z_index, :]/Bk_lin_all[:, z_index, :] ## ## Unsmoothed Q(k)\ny_vals = Bk_all_smooth[:, z_index, :] ## for B ### Using smooth values for emulation\n# y_ind = np.arange(0, y_vals.shape[1])\ny_ind = k_all\n\n# Train-test split\ntest_indices = [0, 14, 35]\ninput_params= p_all[test_indices]\n# target_vals = Bk_all[:, z_index, :][test_indices]/Bk_lin_all[:, z_index, :][test_indices] ## For Q\ntarget_vals = Bk_all[:, z_index, :][test_indices]\n\n# Load validation data\n\n# Bk_all_val, _, _ = load_boost_data(LIBRARY_BK_FILE_VAL, LIBRARY_ZK_FILE_VAL)\n# target_vals = Bk_all_val[:, z_index, :]/Bk_all_val[:, z_index, :]\n# input_params = load_params(LIBRARY_PARAM_FILE_VAL)\n\ntrain_indices = [i for i in  np.arange(49)] # if i not in test_indices]\np_all_train = p_all[train_indices]\n# y_vals_train = Bk_all[:, z_index, :][train_indices]/Bk_lin_all[:, z_index, :][train_indices] ## For Q\ny_vals_train = Bk_all_smooth[:, z_index, :][train_indices]\n\nprint('Redshift: ' + str(z_all[z_index]))\n\nRedshift: 0.186\n\n\n\nsepia_data = sepia_data_format(p_all_train, y_vals_train, y_ind)\n# sepia_data = sepia_data_by_redshift(redshift=0.01)\n\nprint(p_all_train.shape, y_vals_train.shape, y_ind.shape)\n\n\nprint(sepia_data)\nmodel_filename = '../CubicGalileonEmu/model/multivariate_model_z_index' + str(z_index) \n\n# sepia_model = do_pca(sepia_data, exp_variance=0.95)\nsepia_model_pca = do_pca(sepia_data, exp_variance=0.999)\n\n(49, 5) (49, 768) (768,)\nThis SepiaData instance implies the following:\nThis is a simulator (eta)-only model, y dimension 768\nm  =    49 (number of simulated data)\np  =     1 (number of inputs)\nq  =     5 (number of additional simulation inputs)\npu NOT SET (transformed response dimension); call method create_K_basis \n\n\n\n\nsepia_model = do_gp_train(sepia_model_pca, model_filename)\nplot_train_diagnostics(sepia_model)\n\nStarting tune_step_sizes...\nDefault step sizes:\nbetaU\n[[0.1 0.1 0.1 0.1 0.1]\n [0.1 0.1 0.1 0.1 0.1]\n [0.1 0.1 0.1 0.1 0.1]\n [0.1 0.1 0.1 0.1 0.1]\n [0.1 0.1 0.1 0.1 0.1]\n [0.1 0.1 0.1 0.1 0.1]]\nlamUz\n[[5. 5. 5. 5. 5.]]\nlamWs\n[[100. 100. 100. 100. 100.]]\nlamWOs\n[[100.]]\n\n\nStep size tuning: 100%|██████████| 50/50 [03:25&lt;00:00,  4.11s/it]\n\n\nDone with tune_step_size.\nSelected step sizes:\nbetaU\n[[0.20039483 0.53346802 0.70860777 0.38193646 0.94307793]\n [0.08953042 0.16706645 1.86357302 0.69816811 1.26519576]\n [0.0348878  0.11363801 1.02663463 0.55011076 0.44193215]\n [0.09193158 0.32122614 1.57886469 1.48499257 0.82433635]\n [0.07446145 0.21646594 0.09608453 0.81944166 0.90499841]\n [0.70237919 0.84388779 0.80397463 2.03052796 2.73231983]]\nlamUz\n[[0.86994527 1.06549902 1.00787244 1.28935769 1.47918973]]\nlamWs\n[[ 521.21459654  317.49045268 5266.06119313 5512.02207529 4164.28311237]]\nlamWOs\n[[157.42748451]]\n\n\nMCMC sampling: 100%|██████████| 1000/1000 [03:08&lt;00:00,  5.29it/s]\n\n\nModel saved to ../CubicGalileonEmu/model/multivariate_model_z_index8.pkl\nNo thetas to plot\n\n\n\n\n\n\n\n\n\n\n\nLoad existing model\n\nsepia_model = gp_load(sepia_model_pca, model_filename)\n\nWARNING: make sure this model was instantiated with the same input data as the model corresonding to this saved model info.\n\n\n\n\nSingle-redshift emulation for new cosmological parameters\n\ntest_indices_rand = np.random.choice(input_params.shape[0], size=3, replace=False)\npred_mean, pred_std = emulate(sepia_model, sepia_data, input_params[test_indices_rand])\nprint(pred_mean.shape)\n# pred_quant == Emulated (0.05, 0.95) quantile\nf = validation_plot(k_all, target_vals[test_indices_rand], pred_mean, pred_std, xy_lims=[2e-2, 1e1, 0.98, 1.35]);\n# f = validation_plot(k_all, target_vals[test_indices_rand], pred_mean, pred_std, xy_lims=[2e-2, 1e1, 0.9, 1.1]);\n\n(768, 3)\n\n\n\n\n\n\n\n\n\n\n\nSensitivity analysis from the emulator\n\nf = sensitivity_plot(k_all, p_all, sepia_model, sepia_data, emulate, PARAM_NAME, xy_lims=[2e-2, 1e1, 0.98, 1.35]);\n# f = sensitivity_plot(k_all, p_all, sepia_model, sepia_data, emulate, PARAM_NAME, xy_lims=[2e-2, 1e1, 0.9, 1.1]); ## Unsmoothed Q(k)\n\n\n\n\n\n\n\n\n\n\nMulti-redshift emulation\n\nTrain all the models\n\nif if_train_all:\n    \n    do_gp_train_multiple(model_dir='../CubicGalileonEmu/model/', \n                        p_train_all = p_all[train_indices],\n                        # y_vals_all = Bk_all[train_indices]/Bk_lin_all[train_indices] ## Unsmoothed Q(k)\n                        y_vals_all = Bk_all_smooth[train_indices],\n                        y_ind_all = k_all,\n                        z_index_range=range(49))\n\n\n\nLoad all trained models\n\nsepia_model_list, sepia_data_list = load_model_multiple(model_dir='../CubicGalileonEmu/model/', \n                                        p_train_all=p_all[train_indices],\n                                        # y_vals_all=Bk_all[train_indices]/Bk_lin_all[train_indices], ## Unsmoothed Q(k)\n                                        y_vals_all=Bk_all_smooth[train_indices],\n                                        y_ind_all=k_all,\n                                        z_index_range=range(49)\n                                        );\n\nNumber of models loaded: 49 from: ../CubicGalileonEmu/model/\n\n\n\n#### Emulator uncertainty across parameter range\n\n\ninput_params0 = input_params[0]\nz_inputs = 0.05\ninput_params_and_redshift = np.append(input_params0, z_inputs)\nprint(input_params_and_redshift[np.newaxis, :])\n\n# sepia_data_select = sepia_data_by_redshift(redshift=z_inputs)\n# sepia_data_select1 = sepia_data_by_redshift(redshift=z_all[2])\n# sepia_data_select2 = sepia_data_by_redshift(redshift=z_all[3])\n\n\n\nsepia_data_select = sepia_data_list[23]\nsepia_data_select1 = sepia_data_list[22]\nsepia_data_select2 = sepia_data_list[24]\n\nemulated_with_redshift, emulated_with_redshift_err = emu_redshift(input_params_and_redshift[np.newaxis, :], sepia_model_list, sepia_data_list, z_all)\n## There is an unknown issue with z_index=5 model, (sepia_model_list[6])\n# emulate(sepia_model_list[6], sepia_data_select, input_params_and_redshift[:-1])[0]\n\nplt.figure(433)\nplt.plot(k_all, emulated_with_redshift[:, 0], label='interp at z=%.4f'%input_params_and_redshift[-1], lw=5, ls='--')\nplt.plot(k_all, emulate(sepia_model_list[22], sepia_data_select1, input_params_and_redshift[:-1])[0], label='grid z=%.4f'%z_all[2])\nplt.plot(k_all, emulate(sepia_model_list[24], sepia_data_select2, input_params_and_redshift[:-1])[0], label='grid z=%.4f'%z_all[3])\nplt.legend()\nplt.title('Comparison of redshift-space interpolation')\n# plt.plot(k_all, emulate(sepia_model_list[0], input_params))\n# plt.plot(k_all, emulate(sepia_model_list[0], input_params))\n\nplt.show()\n\n[[0.31051392 1.085      2.541      0.67373333 0.480012   0.05      ]]\n\n\n\n\n\n\n\n\n\n\nlen(emulate(sepia_model_list[8], sepia_data, input_params_and_redshift[:-1]))\n\n### Will give an error:\n### ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 4 is different from 5)\n### This happens when the sepia_model and sepia_data do not match\n\n2\n\n\n\n\nEmulator confidence across parameter range\n\n# Parameter settings\nsteps = 50  # Number of steps in the grid for each parameter\nparam_name_extended = np.append(PARAM_NAME, 'Redshift')\nred_min = 0\nred_max = 3\nred_mean = 1.0\n\nparam_min = np.append(p_all.min(axis=0), red_min)\nparam_max = np.append(p_all.max(axis=0), red_max)\nparam_mean = np.append(p_all.mean(axis=0), red_mean)\n\n# Compute outputs and errors for a range of parameter values\ndef compute_errors(param_grid):\n    # print(param_grid.shape)\n    errors = np.zeros(shape=(param_grid.shape[0], ))\n    # errors = np.array([np.mean(emu_redshift(params[np.newaxis, :], sepia_model_list, z_all)[1][:, 0, :]**2) for params in param_grid])\n    for par_indx in range(errors.shape[0]):\n        emu_pred = np.array(emu_redshift(param_grid[par_indx][np.newaxis, :], sepia_model_list, sepia_data_list, z_all))\n        errors_emu = emu_pred[1, :, 0]\n        # print(errors_emu.shape)\n\n        errors[par_indx] = np.max(errors_emu)\n        # print(errors_emu)\n\n    return errors.reshape(steps, steps)\n\n\n# Example usage:\nparam_indices = [4, 2]  # Indices of parameters to vary\nfixed_indices = [i for i in range(len(param_name_extended)) if i not in param_indices]\nfixed_params = {param_name_extended[i]: param_mean[i] for i in fixed_indices}\n\nparam_grid = generate_param_grid_with_fixed(param_name_extended, param_indices, fixed_params, param_min, param_max, steps)\n\nerrors = compute_errors(param_grid)\n\n\nf = plot_error_heatmap( errors, \n                       [param_name_extended[i] for i in param_indices], \n                       [(param_min[param_indices[0]], param_max[param_indices[0]]), (param_min[param_indices[1]], param_max[param_indices[1]])]\n                       )\n\nif if_savefig: \n    f.savefig('../../../Plots/heatmap_params_4_5.png', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\n\nParameter inference via MCMC using the emulator\n\nndim = 5\nnwalkers = 100  # 500\nnrun_burn = 100  # 300\nnrun = 700  # 1000\n\n\nCreating mock observational data\n\ntarget_indx = 0 #0 ,14, 35\nz_index = 2\nL = 32\nfake_obs_data_index_every = 1\n\nredshift = z_all[z_index]\nk_cond = k_all &lt; 1\nx = k_all[::fake_obs_data_index_every][k_cond]\ny = Bk_all[:, z_index, :][target_indx][::fake_obs_data_index_every][k_cond]\ny = y + 1e-4*np.random.normal(0.0, 1.0, size=y.shape)\nyerr = np.sqrt( Bk_all[:, z_index, :][target_indx][::fake_obs_data_index_every][k_cond]*(L**3))/(L**3)\n\nx_grid = k_all\nparams_calib = p_all[target_indx][:, np.newaxis].T\nprint('redshift: ', redshift)\n\nredshift:  0.041\n\n\n\nf, a = plt.subplots(1,1, figsize = (8, 5)) \ninput_params_and_redshift = np.append(params_calib, redshift)\nbk_target, err_target = emu_redshift(input_params_and_redshift[np.newaxis, :], sepia_model_list, sepia_data_list, z_all)\na.plot(k_all, bk_target[:, 0], label='Emulated at target params', lw=5, ls='--')\na.errorbar(x, y, yerr, label='Target mock observations', ls='none', lw=1, color = \"r\")\na.scatter(x, y, s = 5, marker = \"h\", color = \"r\")\n\n\na.plot(k_all, Bk_all[:, z_index, :].T, 'k', alpha=0.1)\n\n\nplt.plot(k_all, emulate(sepia_model_list[z_index], sepia_data_list[z_index], input_params_and_redshift[:-1])[0], label='grid z=%.4f'%z_all[z_index])\nplt.plot(k_all, emulate(sepia_model_list[z_index+1], sepia_data_list[z_index + 1], input_params_and_redshift[:-1])[0], label='grid z=%.4f'%z_all[z_index + 1])\n\nstring_print0 = 'Target Params \\n\\n' \nstring_print1 = PARAM_NAME[0] + '= %.4f'%input_params_and_redshift[0] + '\\n'\nstring_print2 = PARAM_NAME[1] + '= %.4f'%input_params_and_redshift[1] + '\\n'\nstring_print3 = PARAM_NAME[2] + '= %.4f'%input_params_and_redshift[2] + '\\n'\nstring_print4 = PARAM_NAME[3] + '= %.4f'%input_params_and_redshift[3] + '\\n'\nstring_print5 = PARAM_NAME[4] + '= %.4f'%input_params_and_redshift[4] + '\\n'\nstring_print6 = 'redshift' + '= %.4f'%input_params_and_redshift[5] \n\n\nstring_print = string_print0 + string_print1 + string_print2 + string_print3 + string_print4 + string_print5 + string_print6\n\nprops = dict(boxstyle='round', facecolor='gray', alpha=0.2)\nplt.text(1.02, 0.1, string_print, transform=a.transAxes, fontsize=12, bbox=props)\n\n\na.set_xscale('log')\nplt.title('pre-MCMC')\na.set_xlabel(r'$k [h/Mpc]$')\na.set_ylabel(r'$B(k)$')\nplt.legend()\n\n\n\n\n\n\n\n\n\nallMax = np.max(p_all, axis = 0)\nallMin = np.min(p_all, axis = 0)\n\nparam1 = [PARAM_NAME[0], params_calib[0][0], allMin[0], allMax[0]] \nparam2 = [PARAM_NAME[1], params_calib[0][1], allMin[1], allMax[1]]\nparam3 = [PARAM_NAME[2], params_calib[0][2], allMin[2], allMax[2]]\nparam4 = [PARAM_NAME[3], params_calib[0][3], allMin[3], allMax[2]]\nparam5 = [PARAM_NAME[4], params_calib[0][4], allMin[4], allMax[4]]\n\nparams_list = [param1, param2, param3, param4, param5]\n\n\npos0 = chain_init(params_list, ndim, nwalkers)\nsampler = define_sampler(redshift, ndim, nwalkers, params_list, x_grid, sepia_model_list, sepia_data_list, z_all, x, y, yerr)\n\n\nMCMC run - first burn, then full.\n\npos, prob, state, samples, sampler, autocorr, index = do_mcmc(sampler, pos0, nrun_burn, ndim, if_burn=True)\n\nif if_mcmc_all: # Full MCMC-run, will be slow\n    pos, prob, state, samples, sampler, autocorr, index = do_mcmc(sampler, pos, nrun, ndim, if_burn=False)\n\np_mcmc = mcmc_results(samples)\n\nfig = plot_mcmc(samples, params_list, if_truth_know=True)\nif if_savefig: \n    plt.savefig('../../../Plots/mcmc_plot.png', bbox_inches='tight')\n\nBurn-in phase\ntime (minutes): 6.990116775035858\n\n\n100%|██████████| 100/100 [07:20&lt;00:00,  4.40s/it]\n\n\nmcmc results: 0.3117407443412698 1.0816159482641008 2.541760307153967 0.6857972480657628 0.47717583037390404\n\n\n\n\n\n\n\n\n\n\nf, a = plt.subplots(1,1, figsize = (8, 5)) \ninput_params_and_redshift = np.append(p_mcmc, redshift)\nbk_mcmc, err_mcmc = emu_redshift(input_params_and_redshift[np.newaxis, :], sepia_model_list, sepia_data_list, z_all)\na.plot(k_all, bk_mcmc[:, 0], label='Emulated at best MCMC', lw=3, ls='--')\na.errorbar(x, y, yerr, label='Mock target', ls='none', lw=1, color = \"r\")\na.scatter(x, y, s = 5, marker = \"h\", color = \"r\", alpha=0.5)\n\na.plot(k_all, Bk_all[:, z_index, :].T, 'k', alpha=0.1)\n\n\n# plt.plot(k_all, emulate(sepia_model_list[z_index], input_params_and_redshift[:-1])[0], label='Z1')\n# plt.plot(k_all, emulate(sepia_model_list[z_index+1], input_params_and_redshift[:-1])[0], label='Z2')\n\nstring_print0 = 'Target Params \\n\\n' \nstring_print1 = PARAM_NAME[0] + '= %.3f'%params_calib[0][0] + '\\n'\nstring_print2 = PARAM_NAME[1] + '= %.3f'%params_calib[0][1] + '\\n'\nstring_print3 = PARAM_NAME[2] + '= %.3f'%params_calib[0][2] + '\\n'\nstring_print4 = PARAM_NAME[3] + '= %.3f'%params_calib[0][3] + '\\n'\nstring_print5 = PARAM_NAME[4] + '= %.3f'%params_calib[0][4] + '\\n'\nstring_print6 = 'redshift' + '= %.3f'%redshift\n\n\nstring_print = string_print0 + string_print1 + string_print2 + string_print3 + string_print4 + string_print5 + string_print6\n\nprops = dict(boxstyle='round', facecolor='gray', alpha=0.2)\nplt.text(1.02, 0.5, string_print, transform=a.transAxes, fontsize=12, bbox=props)\n\nstring_print0_mcmc = 'Optimized Params \\n\\n' \nstring_print1_mcmc = PARAM_NAME[0] + '= %.3f'%p_mcmc[0] + '\\n'\nstring_print2_mcmc = PARAM_NAME[1] + '= %.3f'%p_mcmc[1] + '\\n'\nstring_print3_mcmc = PARAM_NAME[2] + '= %.3f'%p_mcmc[2] + '\\n'\nstring_print4_mcmc = PARAM_NAME[3] + '= %.3f'%p_mcmc[3] + '\\n'\nstring_print5_mcmc = PARAM_NAME[4] + '= %.3f'%p_mcmc[4] \n\nstring_print_mcmc = string_print0_mcmc + string_print1_mcmc + string_print2_mcmc + string_print3_mcmc + string_print4_mcmc + string_print5_mcmc\n\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nplt.text(1.02, 0.05, string_print_mcmc, transform=a.transAxes, fontsize=12, bbox=props)\n\n\n\na.set_xscale('log')\nplt.title('B(k) at MCMC constraints')\na.set_xlabel(r'$k [h/Mpc]$')\na.set_ylabel(r'$B(k)$')\nplt.legend()\n\nif if_savefig: \n    plt.savefig('../../../Plots/mcmc_results_Bk.png', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nif if_mcmc_all:\n    tau = sampler.get_autocorr_time(tol=0)\n    print(tau)\n\n    plt.figure(43)\n    plt.plot(prob)\n    if if_savefig: \n        plt.savefig('../../../Plots/prob_plot.png', bbox_inches='tight')\n\n    selected_indices_for_plot = [0, 2, 4]\n    fig = plot_mcmc(samples[:, selected_indices_for_plot], [params_list[i] for i in selected_indices_for_plot], if_truth_know=True)\n\n    if if_savefig: \n        plt.savefig('../../../Plots/mcmc_plot_reduced_params.png', bbox_inches='tight')\n\n\ndef plot_convergence(sampler, params_list, nrun, ndim, nwalkers):\n    n_params = len(params_list)  # Number of parameters\n    fig, ax = plt.subplots(n_params, 1, figsize=(20, 2 * n_params), sharex=True)\n    ax[-1].set_xlabel('steps')\n\n    for i, param in enumerate(params_list):\n        ax[i].plot(np.arange(nrun), sampler.chain[:, :, i].T, lw=0.2, alpha=0.9)\n        ax[i].text(0.9, 0.9, param[0], horizontalalignment='center', verticalalignment='center', transform=ax[i].transAxes, fontsize=12)\n\n    # fig.savefig('plots/convergence_mcmc_ndim{}_nwalk{}_run{}_{}-{}.png'.format(ndim, nwalkers, nrun, summary_stat, design), dpi=100)\n\n    return fig\n\nif False:\n    # Example usage\n    plot_convergence(sampler, params_list, nrun, ndim, nwalkers)\n\n\nif if_mcmc_all:\n\n    n = 100 * np.arange(1, index + 1)\n    y = autocorr[:index]\n    plt.plot(n, n / 100.0, \"--k\")\n    plt.plot(n, y)\n    plt.xlim(0, n.max())\n    plt.ylim(0, y.max() + 0.1 * (y.max() - y.min()))\n    plt.xlabel(\"number of steps\")\n    plt.ylabel(r\"mean $\\hat{\\tau}$\");\n    plt.show()\n\n\nif if_mcmc_all:\n    plt.plot(autocorr)\n    plt.xscale('log')\n    # plt.yscale('log')\n\n\n### TO-DO\n\n## Redshift sampling is better in the new Validation design -- validate AFTER redshift interpolation\n## New heatmap with max errors\n## Use smoothing for spectra\n## Use Carola's new data\n## MCMC with all the accumulated errors\n## Add cosmicEmu's P(k) for MCMC\n## Carola will try P(k) + cosmic Shear?\n## Fisher ellipses?",
    "crumbs": [
      "CubicGalileonEmu"
    ]
  },
  {
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "viz",
    "section": "",
    "text": "source\n\nplot_lines_with_param_color\n\n plot_lines_with_param_color (param_array:&lt;built-infunctionarray&gt;=None,\n                              x_array:&lt;built-infunctionarray&gt;=None,\n                              y_array_all:&lt;built-infunctionarray&gt;=None,\n                              title_str:str=None, xlabel_str:str=None,\n                              ylabel_str:str=None,\n                              param_name_str:str=None,\n                              ax:matplotlib.axes._axes.Axes=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparam_array\narray\nNone\nparameter array\n\n\nx_array\narray\nNone\nx-axis array\n\n\ny_array_all\narray\nNone\ny-axis array\n\n\ntitle_str\nstr\nNone\nTitle string\n\n\nxlabel_str\nstr\nNone\nx-label string\n\n\nylabel_str\nstr\nNone\ny-label string\n\n\nparam_name_str\nstr\nNone\nParameter string,\n\n\nax\nAxes\nNone\n\n\n\n\n\nsource\n\n\nplot_scatter_matrix\n\n plot_scatter_matrix (df:pandas.core.frame.DataFrame=None,\n                      colors:str=None)\n\n\nsource\n\n\nplot_train_diagnostics\n\n plot_train_diagnostics (sepia_model:sepia.SepiaModel.SepiaModel=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format, after PCA\n\n\nReturns\ntuple\n\nPair-plot and Trace-plot\n\n\n\n\nsource\n\n\nsensitivity_plot\n\n sensitivity_plot (k_all:&lt;built-infunctionarray&gt;=None, params_all:&lt;built-\n                   infunctionarray&gt;=None,\n                   sepia_model:sepia.SepiaModel.SepiaModel=None,\n                   sepia_data:sepia.SepiaData.SepiaData=None,\n                   emulator_function=None, param_name:tuple=None,\n                   xy_lims:&lt;built-infunctionarray&gt;=[0.02, 10.0, 0.98,\n                   1.3])\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk_all\narray\nNone\nall wavenumbers\n\n\nparams_all\narray\nNone\nall parameters\n\n\nsepia_model\nSepiaModel\nNone\nSEPIA emulator model\n\n\nsepia_data\nSepiaData\nNone\nInput data in SEPIA format\n\n\nemulator_function\nNoneType\nNone\nfunction which takes in sepia model and parameters\n\n\nparam_name\ntuple\nNone\nParameter name\n\n\nxy_lims\narray\n[0.02, 10.0, 0.98, 1.3]\n\n\n\n\n\nsource\n\n\nvalidation_plot\n\n validation_plot (k_all:&lt;built-infunctionarray&gt;=None, target_vals:&lt;built-\n                  infunctionarray&gt;=None, pred_mean:&lt;built-\n                  infunctionarray&gt;=None, pred_std:&lt;built-\n                  infunctionarray&gt;=None, xy_lims:&lt;built-\n                  infunctionarray&gt;=[0.02, 10.0, 0.98, 1.3])\n\n\nsource\n\n\nplot_mcmc\n\n plot_mcmc (samples:&lt;built-infunctionarray&gt;, params_list:list,\n            if_truth_know:bool=False)\n\n\n# #| hide\n\n# from getdist import plots, MCSamples\n# import re\n\n# def latex_to_plain(text):\n#     # Replace LaTeX specific symbols\n#     text = re.sub(r'\\$', '', text)  # Remove $ signs\n#     text = re.sub(r'\\\\', '', text)  # Remove backslashes\n#     text = re.sub(r'\\{|\\}', '', text)  # Remove braces\n#     return text\n\n\n# def plot_mcmc_getdist(samples:np.array, \n#               params_list:list, \n#               if_truth_know:bool=False):\n\n\n#     # from getdist.gaussian_mixtures import GaussianND\n#     # mat = - h\n#     # cov = np.linalg.pinv(mat)\n\n#     # covariance = cov #[[0.001**2, 0.0006*0.05, 0], [0.0006*0.05, 0.05**2, 0.2**2], [0, 0.2**2, 2**2]]\n#     # mean =np.array([para4[1], para5[1]]) #params #[0.02, 1, -2] \n#     # gauss=GaussianND(mean, covariance, names=['logfr0','n'], labels=[para4[0], para5[0]], label = 'Fisher')\n#     # # g = plots.get_subplot_plotter()\n#     # # g.triangle_plot(gauss,filled=True)\n\n#     # names = ['a', 'b', 'c', 'd', 'e']\n#     # s1 = samples_plot[samples_plot[:, 0] &gt; -5.3]\n\n#     s1 = samples\n#     param_names = [param[0] for param in params_list]\n#     PARAM_NAME_trial = np.array([latex_to_plain(item) for item in param_names]) #['Omega_m', 'n_s', '10^{9} A_s', 'h', 'f_\\phi']\n\n#     samples1 = MCSamples(samples=s1, \n#                         names=PARAM_NAME_trial, \n#                         labels=PARAM_NAME_trial, \n#                         label='MCMC' \n#                         )#, ranges={'logfr0':(-5.2, -4.8), 'n':(0.5, 1.5)})\n#     g = plots.get_subplot_plotter(subplot_size=4)\n#     g.settings.axes_fontsize=27\n#     g.settings.axes_labelsize = 27\n#     g.settings.legend_fontsize = 27\n#     g.settings.fontsize = 27\n#     g.settings.alpha_filled_add=0.6\n#     # g.settings.title_limit_fontsize = 27\n#     g.settings.solid_contour_palefactor = 0.5\n#     g.triangle_plot([samples1],  \n#                     # ['logfr0','n'], \n#                     filled=True, \n#                     # markers={'logfr0':para4[1], 'n':para5[1]}, \n#                     markers=[param[1] for param in params_list],\n#                     marker_args={'lw':2, 'ls':'dashed', 'color':'k'}\n#                     )\n\n#     # g.triangle_plot([samples1, gauss],  ['logfr0','n'], filled=True, markers={'logfr0':para4[1], 'n':para5[1]}, marker_args={'lw':2, 'ls':'dashed', 'color':'k'})\n#     # g.plot_contours(h, params, fill=True, alpha=0.5, label = 'Fisher info', facecolor = 'red')\n\n#     # g.export('../../../Plots/triangle_plot.png')\n\n\nsource\n\n\ngenerate_param_grid_with_fixed\n\n generate_param_grid_with_fixed (param_name:list=None,\n                                 param_indices:&lt;built-\n                                 infunctionarray&gt;=None,\n                                 fixed_params:&lt;built-\n                                 infunctionarray&gt;=None, param_min:&lt;built-\n                                 infunctionarray&gt;=None, param_max:&lt;built-\n                                 infunctionarray&gt;=None, steps:int=40)\n\n\nsource\n\n\nplot_error_heatmap\n\n plot_error_heatmap (errors:&lt;built-infunctionarray&gt;=None,\n                     param_names:list=None, param_range:tuple=None)",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "emu.html",
    "href": "emu.html",
    "title": "emu",
    "section": "",
    "text": "# #| export\n\n# def blockPrint():\n#     sys._jupyter_stdout = sys.stdout\n#     sys.stdout = open(os.devnull, 'w')\n\n# def enablePrint():\n#     sys._jupyter_stdout = sys.stdout\n#     sys.stdout = sys.__stdout__\n\n\nsource\n\nenablePrint\n\n enablePrint ()\n\n\nsource\n\n\nblockPrint\n\n blockPrint ()\n\n\n# #| hide\n\n# def emulate_old(sepia_model:SepiaModel=None, # Input data in SEPIA format\n#         input_params:np.array=None, #Input parameter array \n#        ) -&gt; tuple: # 2 np.array of mean and (0.05,0.95) quantile in prediction\n    \n    \n#     if len(input_params.shape) == 1:\n#         ip = np.expand_dims(input_params, axis=0)\n        \n#     else:\n#         ip = input_params\n        \n#     pred_samples= sepia_model.get_samples(numsamples=20)\n        \n#     pred = SepiaEmulatorPrediction(t_pred=ip, samples=pred_samples, model=sepia_model, storeMuSigma=True)\n    \n#     pred_samps = pred.get_y()\n#     pred_mean = np.mean(pred_samps, axis=0).T\n#     pred_quant = np.quantile(pred_samps, [0.05, 0.95], axis=0).T\n\n#     return np.array(pred_mean), np.array(pred_quant)\n\n\n# #| hide\n\n# def emulate(sepia_model:SepiaModel=None, # Input data in SEPIA format\n#         input_params:np.array=None, #Input parameter array \n#        ) -&gt; tuple : # 2 np.array of mean and (0.05,0.95) quantile in prediction\n    \n    \n#     if len(input_params.shape) == 1:\n#         ip = np.expand_dims(input_params, axis=0)\n        \n#     else:\n#         ip = input_params\n        \n#     pred_samples= sepia_model.get_samples(numsamples=20)\n        \n#     pred = SepiaEmulatorPrediction(t_pred=ip, samples=pred_samples, model=sepia_model, storeMuSigma=True)\n    \n#     pred_samps = pred.get_y()\n#     pred_mean = np.mean(pred_samps, axis=0).T\n#     pred_err = np.quantile(pred_samps, [0.05, 0.95], axis=0).T\n\n\n\n#     # pred_mean_arr, pred_err_arr = pred.get_mu_sigma()\n#     # pred_mean = np.mean(pred_mean_arr, axis=0).T\n#     # pred_err = np.std(pred_err_arr, axis=0).T\n\n#     '''\n#     pred_mean_arr = pred.mu #(#samples, #x_pred)\n#     pred_err_arr = pred.sigma #(#samples, #x_pred, #x_pred)\n\n\n#     pred_mean = np.mean(pred_mean_arr, axis=0).T\n#     pred_err = np.mean(pred_err_arr, axis=0).T ## pred.diag?\n\n#     print(pred_mean_arr.shape)\n#     print(pred_err_arr.shape)\n    \n#     '''\n\n#     return np.array(pred_mean), np.array(pred_err)\n#     # return np.array(pred_mean), np.array(pred_err)\n\n\nsource\n\n\nemulate\n\n emulate (sepia_model:sepia.SepiaModel.SepiaModel=None,\n          sepia_data:sepia.SepiaData.SepiaData=None, input_params:&lt;built-\n          infunctionarray&gt;=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput model in SEPIA format\n\n\nsepia_data\nSepiaData\nNone\nInput data in SEPIA format\n\n\ninput_params\narray\nNone\nInput parameter array\n\n\nReturns\ntuple\n\n2 np.array of mean and std\n\n\n\n\n# #| export\n\n# def load_model_multiple(model_dir:str=None, # Pickle directory path\n#                         p_train_all:np.array=None, # Parameter array\n#                         y_vals_all:np.array=None, # Target y-values array\n#                         y_ind_all:np.array=None, # x-values\n#                         z_index_range:np.array=None, # Snapshot indices for training\n#                    ) -&gt; None: \n    \n#     blockPrint()\n    \n#     model_list = []\n#     data_list = []\n    \n#     for z_index in z_index_range:\n        \n#         sepia_data = sepia_data_format(p_train_all, y_vals_all[:, z_index, :], y_ind_all)\n        \n#         sepia_model_pca_i = do_pca(sepia_data, exp_variance=0.999)\n        \n#         model_filename = model_dir + 'multivariate_model_z_index' + str(z_index) \n#         sepia_model_z = gp_load(sepia_model_pca_i, model_filename)\n#         model_list.append(sepia_model_z)\n#         data_list.append(sepia_data)\n\n#     enablePrint()\n\n#     print('Number of models loaded: ' + str(len(model_list)) + ' from: ' + model_dir  )\n\n    \n#     '''\n#     # def sepia_data_by_redshift(redshift):\n#     ## Data prep\n#     Bk_all, k_all, z_all = load_boost_data()\n#     p_all = load_params()\n#     z_index = np.argmin(np.abs(z_all - redshift))\n#     print('Redshift index: ' + str(z_index))\n\n#     # Load validation data\n#     train_indices = [i for i in  np.arange(49)] # if i not in test_indices]\n#     p_all_train = p_all[train_indices]\n#     y_vals_train = Bk_all[:, z_index, :][train_indices]\n#     print('Redshift: ' + str(z_all[z_index]))\n\n#     sepia_data_z = sepia_data_format(p_all_train, y_vals_train, k_all)\n#     do_pca(sepia_data_z, exp_variance=0.999)\n\n#     return sepia_data_z\n\n#     '''\n\n\n#     return model_list, data_list\n\n\nsource\n\n\nload_model_multiple\n\n load_model_multiple (model_dir:str=None, p_train_all:&lt;built-\n                      infunctionarray&gt;=None, y_vals_all:&lt;built-\n                      infunctionarray&gt;=None, y_ind_all:&lt;built-\n                      infunctionarray&gt;=None, z_index_range:&lt;built-\n                      infunctionarray&gt;=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_dir\nstr\nNone\nPickle directory path\n\n\np_train_all\narray\nNone\nParameter array\n\n\ny_vals_all\narray\nNone\nTarget y-values array\n\n\ny_ind_all\narray\nNone\nx-values\n\n\nz_index_range\narray\nNone\nSnapshot indices for training\n\n\nReturns\nNone\n\n\n\n\n\n\nsource\n\n\nemu_redshift\n\n emu_redshift (input_params_and_redshift:&lt;built-infunctionarray&gt;=None,\n               sepia_model_list:list=None, sepia_data_list:list=None,\n               z_all:&lt;built-infunctionarray&gt;=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_params_and_redshift\narray\nNone\nInput parameters (along with redshift)\n\n\nsepia_model_list\nlist\nNone\n\n\n\nsepia_data_list\nlist\nNone\n\n\n\nz_all\narray\nNone\nAll the trained models",
    "crumbs": [
      "emu"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "pca",
    "section": "",
    "text": "source\n\ndo_pca\n\n do_pca (sepia_data:sepia.SepiaData.SepiaData=None,\n         exp_variance:float=0.999, do_discrepancy:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_data\nSepiaData\nNone\nInput data in SEPIA format\n\n\nexp_variance\nfloat\n0.999\nExplained variance\n\n\ndo_discrepancy\nbool\nFalse\nFor discrepancy modeling\n\n\nReturns\nSepiaModel\n\nsepia.SepiaModel.SepiaModel",
    "crumbs": [
      "pca"
    ]
  },
  {
    "objectID": "mcmc.html",
    "href": "mcmc.html",
    "title": "mcmc",
    "section": "",
    "text": "source\n\nln_prior\n\n ln_prior (theta, params_list)\n\n\nsource\n\n\nln_like\n\n ln_like (theta, redshift, x_grid, sepia_model_list, sepia_data_list,\n          z_all, x, y, yerr)\n\n\nsource\n\n\nln_prob\n\n ln_prob (theta, redshift, params_list, x_grid, sepia_model_list,\n          sepia_data_list, z_all, x, y, yerr)\n\n\nsource\n\n\nchain_init\n\n chain_init (params_list, ndim, nwalkers)\n\n\nsource\n\n\ndefine_sampler\n\n define_sampler (redshift, ndim, nwalkers, params_list, x_grid,\n                 sepia_model_list, sepia_data_list, z_all, x, y, yerr)\n\n\nsource\n\n\ndo_mcmc\n\n do_mcmc (sampler, pos, nrun, ndim, if_burn=False)\n\n\nsource\n\n\nmcmc_results\n\n mcmc_results (samples)",
    "crumbs": [
      "mcmc"
    ]
  },
  {
    "objectID": "gp.html",
    "href": "gp.html",
    "title": "gp",
    "section": "",
    "text": "source\n\ndo_gp_train\n\n do_gp_train (sepia_model:sepia.SepiaModel.SepiaModel=None,\n              model_file:str=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format, after PCA\n\n\nmodel_file\nstr\nNone\npickle file path\n\n\nReturns\nSepiaModel\n\nsepia.SepiaModel.SepiaModel after GP\n\n\n\n\nsource\n\n\ngp_load\n\n gp_load (sepia_model:sepia.SepiaModel.SepiaModel=None, model_file:str='/h\n          ome/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonE\n          mu/modelmultivariate_model')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format (Pre-PCA is fine? – CHECK)\n\n\nmodel_file\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/modelmultivariate_model\npickle file path\n\n\nReturns\nSepiaModel\n\nsepia.SepiaModel.SepiaModel\n\n\n\n\nsource\n\n\ngp_load_all\n\n gp_load_all ()\n\n\nsource\n\n\ndo_gp_train_multiple\n\n do_gp_train_multiple (model_dir:str=None, p_train_all:&lt;built-\n                       infunctionarray&gt;=None, y_vals_all:&lt;built-\n                       infunctionarray&gt;=None, y_ind_all:&lt;built-\n                       infunctionarray&gt;=None, z_index_range:&lt;built-\n                       infunctionarray&gt;=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_dir\nstr\nNone\nPickle directory path\n\n\np_train_all\narray\nNone\nParameter array\n\n\ny_vals_all\narray\nNone\nTarget y-values array\n\n\ny_ind_all\narray\nNone\nx-values\n\n\nz_index_range\narray\nNone\nSnapshot indices for training\n\n\nReturns\nNone",
    "crumbs": [
      "gp"
    ]
  }
]