[
  {
    "objectID": "emu.html",
    "href": "emu.html",
    "title": "emu",
    "section": "",
    "text": "source\n\nenablePrint\n\n enablePrint ()\n\n\nsource\n\n\nblockPrint\n\n blockPrint ()\n\n\n# #| hide\n\n# def emulate_old(sepia_model:SepiaModel=None, # Input data in SEPIA format\n#         input_params:np.array=None, #Input parameter array \n#        ) -&gt; tuple: # 2 np.array of mean and (0.05,0.95) quantile in prediction\n    \n    \n#     if len(input_params.shape) == 1:\n#         ip = np.expand_dims(input_params, axis=0)\n        \n#     else:\n#         ip = input_params\n        \n#     pred_samples= sepia_model.get_samples(numsamples=20)\n        \n#     pred = SepiaEmulatorPrediction(t_pred=ip, samples=pred_samples, model=sepia_model, storeMuSigma=True)\n    \n#     pred_samps = pred.get_y()\n#     pred_mean = np.mean(pred_samps, axis=0).T\n#     pred_quant = np.quantile(pred_samps, [0.05, 0.95], axis=0).T\n\n#     return np.array(pred_mean), np.array(pred_quant)\n\n\n# #| export\n\n# def emulate(sepia_model:SepiaModel=None, # Input data in SEPIA format\n#         input_params:np.array=None, #Input parameter array \n#        ) -&gt; tuple : # 2 np.array of mean and (0.05,0.95) quantile in prediction\n    \n    \n#     if len(input_params.shape) == 1:\n#         ip = np.expand_dims(input_params, axis=0)\n        \n#     else:\n#         ip = input_params\n        \n#     pred_samples= sepia_model.get_samples(numsamples=20)\n        \n#     pred = SepiaEmulatorPrediction(t_pred=ip, samples=pred_samples, model=sepia_model, storeMuSigma=True)\n    \n#     pred_samps = pred.get_y()\n#     pred_mean = np.mean(pred_samps, axis=0).T\n#     pred_err = np.quantile(pred_samps, [0.05, 0.95], axis=0).T\n\n\n\n#     # pred_mean_arr, pred_err_arr = pred.get_mu_sigma()\n#     # pred_mean = np.mean(pred_mean_arr, axis=0).T\n#     # pred_err = np.std(pred_err_arr, axis=0).T\n\n#     '''\n#     pred_mean_arr = pred.mu #(#samples, #x_pred)\n#     pred_err_arr = pred.sigma #(#samples, #x_pred, #x_pred)\n\n\n#     pred_mean = np.mean(pred_mean_arr, axis=0).T\n#     pred_err = np.mean(pred_err_arr, axis=0).T ## pred.diag?\n\n#     print(pred_mean_arr.shape)\n#     print(pred_err_arr.shape)\n    \n#     '''\n\n#     return np.array(pred_mean), np.array(pred_err)\n#     # return np.array(pred_mean), np.array(pred_err)\n\n\n'''\n#| hide\n\ndef emulate(sepia_model: SepiaModel = None,  # Input model in SEPIA format\n                 sepia_data: SepiaData = None,  # Input data in SEPIA format\n                 input_params: np.array = None #Input parameter array \n                 ) -&gt; tuple: # 2 np.array of mean and std\n    \n    if input_params.ndim == 1:\n        input_params = np.expand_dims(input_params, axis=0)\n    \n    # Fetch prediction samples once, assuming it can be reused\n    pred_samples = sepia_model.get_samples(numsamples=1)\n\n    # Assuming SepiaEmulatorPrediction can process batch inputs\n    preds = [SepiaEmulatorPrediction(t_pred=np.expand_dims(param, axis=0), samples=pred_samples, model=sepia_model, storeMuSigma=True) \n             for param in input_params]\n    \n    # Extract mu and sigma in a batch-wise fashion\n    pred_means = np.array([pred.mu[0] for pred in preds])\n    pred_sigmas = np.array([np.diag(pred.sigma[0]) for pred in preds])\n    \n    # Dot product in batch: mu_dot_K\n    K_matrix = sepia_data.sim_data.K\n    orig_y_sd, orig_y_mean = sepia_data.sim_data.orig_y_sd, sepia_data.sim_data.orig_y_mean\n\n    mu_dot_K = pred_means[:, :K_matrix.shape[0]] @ K_matrix\n    std_dot_K = np.sqrt(pred_sigmas[:, :K_matrix.shape[0]]) @ K_matrix\n    \n    # Rescaling in a single operation\n    mu_dot_K_rescaled = orig_y_sd * mu_dot_K + orig_y_mean\n    std_dot_K_rescaled = orig_y_sd * std_dot_K + orig_y_mean\n\n    # return np.array(preds), mu_dot_K_rescaled, std_dot_K_rescaled\n    return mu_dot_K_rescaled.T, std_dot_K_rescaled.T\n\n\n'''\n\n'\\n#| hide\\n\\ndef emulate(sepia_model: SepiaModel = None,  # Input model in SEPIA format\\n                 sepia_data: SepiaData = None,  # Input data in SEPIA format\\n                 input_params: np.array = None #Input parameter array \\n                 ) -&gt; tuple: # 2 np.array of mean and std\\n    \\n    if input_params.ndim == 1:\\n        input_params = np.expand_dims(input_params, axis=0)\\n    \\n    # Fetch prediction samples once, assuming it can be reused\\n    pred_samples = sepia_model.get_samples(numsamples=1)\\n\\n    # Assuming SepiaEmulatorPrediction can process batch inputs\\n    preds = [SepiaEmulatorPrediction(t_pred=np.expand_dims(param, axis=0), samples=pred_samples, model=sepia_model, storeMuSigma=True) \\n             for param in input_params]\\n    \\n    # Extract mu and sigma in a batch-wise fashion\\n    pred_means = np.array([pred.mu[0] for pred in preds])\\n    pred_sigmas = np.array([np.diag(pred.sigma[0]) for pred in preds])\\n    \\n    # Dot product in batch: mu_dot_K\\n    K_matrix = sepia_data.sim_data.K\\n    orig_y_sd, orig_y_mean = sepia_data.sim_data.orig_y_sd, sepia_data.sim_data.orig_y_mean\\n\\n    mu_dot_K = pred_means[:, :K_matrix.shape[0]] @ K_matrix\\n    std_dot_K = np.sqrt(pred_sigmas[:, :K_matrix.shape[0]]) @ K_matrix\\n    \\n    # Rescaling in a single operation\\n    mu_dot_K_rescaled = orig_y_sd * mu_dot_K + orig_y_mean\\n    std_dot_K_rescaled = orig_y_sd * std_dot_K + orig_y_mean\\n\\n    # return np.array(preds), mu_dot_K_rescaled, std_dot_K_rescaled\\n    return mu_dot_K_rescaled.T, std_dot_K_rescaled.T\\n\\n\\n'\n\n\n\nsource\n\n\nemulate\n\n emulate (sepia_model:sepia.SepiaModel.SepiaModel=None,\n          sepia_data:sepia.SepiaData.SepiaData=None, input_params:&lt;built-\n          infunctionarray&gt;=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput model in SEPIA format\n\n\nsepia_data\nSepiaData\nNone\nInput data in SEPIA format\n\n\ninput_params\narray\nNone\nInput parameter array\n\n\nReturns\ntuple\n\n2 np.array of mean and std\n\n\n\n\nsource\n\n\nload_model_multiple\n\n load_model_multiple (model_dir:str=None, p_train_all:&lt;built-\n                      infunctionarray&gt;=None, y_vals_all:&lt;built-\n                      infunctionarray&gt;=None, y_ind_all:&lt;built-\n                      infunctionarray&gt;=None, z_index_range:&lt;built-\n                      infunctionarray&gt;=None, sepia_model_i:str=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_dir\nstr\nNone\nPickle directory path\n\n\np_train_all\narray\nNone\nParameter array\n\n\ny_vals_all\narray\nNone\nTarget y-values array\n\n\ny_ind_all\narray\nNone\nx-values\n\n\nz_index_range\narray\nNone\nSnapshot indices for training\n\n\nsepia_model_i\nstr\nNone\n\n\n\nReturns\nNone\n\n\n\n\n\n\n'''\n#| export\n\ndef emu_redshift(input_params_and_redshift:np.array=None, # Input parameters (along with redshift) \n                 sepia_model_list:list=None,\n                 sepia_data: SepiaData = None,  # Input data in SEPIA format\n                 z_all:np.array=None): # All the trained models\n    \n    z = input_params_and_redshift[:, -1]\n    input_params = input_params_and_redshift[:, :-1]\n       \n    \n    # if (z == 0):\n    #     # No redshift interpolation for z=0\n    #     GPm, PCAm = model_load(snap_ID=LAST_SNAP, nRankMax=DEFAULT_PCA_RANK)\n    #     Pk_interp = emulate(sepia_model, sepia_data, input_params)\n        \n        \n    # else:\n    \n    \n    # Linear interpolation between z1 &lt; z &lt; z2\n    snap_idx_nearest = (np.abs(z_all - z)).argmin()\n    if (z &gt; z_all[snap_idx_nearest]):\n        snap_ID_z1 = snap_idx_nearest - 1\n    else:\n        snap_ID_z1 = snap_idx_nearest\n    snap_ID_z2 = snap_ID_z1 + 1\n    \n\n    sepia_model_z1 = sepia_model_list[snap_ID_z1]\n    Bk_z1, Bk_z1_err = emulate(sepia_model_z1, sepia_data, input_params)\n    z1 = z_all[snap_ID_z1]\n    \n\n    sepia_model_z2 = sepia_model_list[snap_ID_z2]\n    Bk_z2, Bk_z2_err = emulate(sepia_model_z2, sepia_data, input_params)\n    z2 = z_all[snap_ID_z2]\n\n    Bk_interp = np.zeros_like(Bk_z1)\n    Bk_interp = Bk_z2 + (Bk_z1 - Bk_z2)*(z - z2)/(z1 - z2)\n\n    Bk_interp_err = np.zeros_like(Bk_z1_err)\n    Bk_interp_err = Bk_z2_err + (Bk_z1_err - Bk_z2_err)*(z - z2)/(z1 - z2)\n    \n    return Bk_interp, Bk_interp_err\n\n'''\n\n'\\n#| export\\n\\ndef emu_redshift(input_params_and_redshift:np.array=None, # Input parameters (along with redshift) \\n                 sepia_model_list:list=None,\\n                 sepia_data: SepiaData = None,  # Input data in SEPIA format\\n                 z_all:np.array=None): # All the trained models\\n    \\n    z = input_params_and_redshift[:, -1]\\n    input_params = input_params_and_redshift[:, :-1]\\n       \\n    \\n    # if (z == 0):\\n    #     # No redshift interpolation for z=0\\n    #     GPm, PCAm = model_load(snap_ID=LAST_SNAP, nRankMax=DEFAULT_PCA_RANK)\\n    #     Pk_interp = emulate(sepia_model, sepia_data, input_params)\\n        \\n        \\n    # else:\\n    \\n    \\n    # Linear interpolation between z1 &lt; z &lt; z2\\n    snap_idx_nearest = (np.abs(z_all - z)).argmin()\\n    if (z &gt; z_all[snap_idx_nearest]):\\n        snap_ID_z1 = snap_idx_nearest - 1\\n    else:\\n        snap_ID_z1 = snap_idx_nearest\\n    snap_ID_z2 = snap_ID_z1 + 1\\n    \\n\\n    sepia_model_z1 = sepia_model_list[snap_ID_z1]\\n    Bk_z1, Bk_z1_err = emulate(sepia_model_z1, sepia_data, input_params)\\n    z1 = z_all[snap_ID_z1]\\n    \\n\\n    sepia_model_z2 = sepia_model_list[snap_ID_z2]\\n    Bk_z2, Bk_z2_err = emulate(sepia_model_z2, sepia_data, input_params)\\n    z2 = z_all[snap_ID_z2]\\n\\n    Bk_interp = np.zeros_like(Bk_z1)\\n    Bk_interp = Bk_z2 + (Bk_z1 - Bk_z2)*(z - z2)/(z1 - z2)\\n\\n    Bk_interp_err = np.zeros_like(Bk_z1_err)\\n    Bk_interp_err = Bk_z2_err + (Bk_z1_err - Bk_z2_err)*(z - z2)/(z1 - z2)\\n    \\n    return Bk_interp, Bk_interp_err\\n\\n'\n\n\n\nsource\n\n\nemu_redshift\n\n emu_redshift (input_params_and_redshift:&lt;built-infunctionarray&gt;=None,\n               sepia_model_list:list=None, sepia_data_list:list=None,\n               z_all:&lt;built-infunctionarray&gt;=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_params_and_redshift\narray\nNone\nInput parameters (along with redshift)\n\n\nsepia_model_list\nlist\nNone\n\n\n\nsepia_data_list\nlist\nNone\n\n\n\nz_all\narray\nNone\nAll the trained models",
    "crumbs": [
      "emu"
    ]
  },
  {
    "objectID": "pca.html",
    "href": "pca.html",
    "title": "pca",
    "section": "",
    "text": "source\n\ndo_pca\n\n do_pca (sepia_data:sepia.SepiaData.SepiaData=None,\n         exp_variance:float=0.95, do_discrepancy:bool=False)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_data\nSepiaData\nNone\nInput data in SEPIA format\n\n\nexp_variance\nfloat\n0.95\nExplained variance\n\n\ndo_discrepancy\nbool\nFalse\nFor discrepancy modeling\n\n\nReturns\nSepiaModel\n\nsepia.SepiaModel.SepiaModel",
    "crumbs": [
      "pca"
    ]
  },
  {
    "objectID": "load.html",
    "href": "load.html",
    "title": "load",
    "section": "",
    "text": "source\n\nload_boost_data\n\n load_boost_data (Bk_fileIn:str='/home/runner/work/CubicGalileonEmu/CubicG\n                  alileonEmu/CubicGalileonEmu/data/Boost.npy', Zk_fileIn:s\n                  tr='/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/\n                  CubicGalileonEmu/data/z_k.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nBk_fileIn\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/data/Boost.npy\nInput file for Boost\n\n\nZk_fileIn\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/data/z_k.txt\nInput file for redshift and wavenumbers\n\n\nReturns\ntuple\n\nBoost, wavenumbers, redshifts\n\n\n\n\n# b, k, z = load_boost_training()\n\n\n# b[:, np.argsort(z), :]\n\n# np.argsort(k)\n\n\nsource\n\n\nload_params\n\n load_params (p_fileIn:str='/home/runner/work/CubicGalileonEmu/CubicGalile\n              onEmu/CubicGalileonEmu/data/cosmo_newdesign.txt')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np_fileIn\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/data/cosmo_newdesign.txt\nInput file for parameters\n\n\nReturns\narray\n\nParameters\n\n\n\n\nsource\n\n\nsepia_data_format\n\n sepia_data_format (design:&lt;built-infunctionarray&gt;=None, y_vals:&lt;built-\n                    infunctionarray&gt;=None, y_ind:&lt;built-\n                    infunctionarray&gt;=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndesign\narray\nNone\nParams array of shape (num_simulation, num_params)\n\n\ny_vals\narray\nNone\nShape (num_simulation, num_y_values)\n\n\ny_ind\narray\nNone\nShape (num_y_values,)\n\n\nReturns\nSepiaData\n\nSepia data format",
    "crumbs": [
      "load"
    ]
  },
  {
    "objectID": "gp.html",
    "href": "gp.html",
    "title": "gp",
    "section": "",
    "text": "source\n\ndo_gp_train\n\n do_gp_train (sepia_model:sepia.SepiaModel.SepiaModel=None,\n              model_file:str=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format, after PCA\n\n\nmodel_file\nstr\nNone\npickle file path\n\n\nReturns\nSepiaModel\n\nsepia.SepiaModel.SepiaModel after GP\n\n\n\n\nsource\n\n\ngp_load\n\n gp_load (sepia_model:sepia.SepiaModel.SepiaModel=None, model_file:str='/h\n          ome/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonE\n          mu/modelmultivariate_model')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format (Pre-PCA is fine? – CHECK)\n\n\nmodel_file\nstr\n/home/runner/work/CubicGalileonEmu/CubicGalileonEmu/CubicGalileonEmu/modelmultivariate_model\npickle file path\n\n\nReturns\nSepiaModel\n\nsepia.SepiaModel.SepiaModel\n\n\n\n\nsource\n\n\ngp_load_all\n\n gp_load_all ()\n\n\nsource\n\n\ndo_gp_train_multiple\n\n do_gp_train_multiple (model_dir:str=None, p_train_all:&lt;built-\n                       infunctionarray&gt;=None, y_vals_all:&lt;built-\n                       infunctionarray&gt;=None, y_ind_all:&lt;built-\n                       infunctionarray&gt;=None, z_index_range:&lt;built-\n                       infunctionarray&gt;=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel_dir\nstr\nNone\nPickle directory path\n\n\np_train_all\narray\nNone\nParameter array\n\n\ny_vals_all\narray\nNone\nTarget y-values array\n\n\ny_ind_all\narray\nNone\nx-values\n\n\nz_index_range\narray\nNone\nSnapshot indices for training\n\n\nReturns\nNone",
    "crumbs": [
      "gp"
    ]
  },
  {
    "objectID": "viz.html",
    "href": "viz.html",
    "title": "viz",
    "section": "",
    "text": "source\n\nplot_lines_with_param_color\n\n plot_lines_with_param_color (param_array:&lt;built-infunctionarray&gt;=None,\n                              x_array:&lt;built-infunctionarray&gt;=None,\n                              y_array_all:&lt;built-infunctionarray&gt;=None,\n                              title_str:str=None, xlabel_str:str=None,\n                              ylabel_str:str=None,\n                              param_name_str:str=None,\n                              ax:matplotlib.axes._axes.Axes=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nparam_array\narray\nNone\nparameter array\n\n\nx_array\narray\nNone\nx-axis array\n\n\ny_array_all\narray\nNone\ny-axis array\n\n\ntitle_str\nstr\nNone\nTitle string\n\n\nxlabel_str\nstr\nNone\nx-label string\n\n\nylabel_str\nstr\nNone\ny-label string\n\n\nparam_name_str\nstr\nNone\nParameter string,\n\n\nax\nAxes\nNone\n\n\n\n\n\nsource\n\n\nplot_scatter_matrix\n\n plot_scatter_matrix (df:pandas.core.frame.DataFrame=None,\n                      colors:str=None)\n\n\nsource\n\n\nplot_train_diagnostics\n\n plot_train_diagnostics (sepia_model:sepia.SepiaModel.SepiaModel=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsepia_model\nSepiaModel\nNone\nInput data in SEPIA format, after PCA\n\n\nReturns\ntuple\n\nPair-plot and Trace-plot\n\n\n\n\nsource\n\n\nsensitivity_plot\n\n sensitivity_plot (k_all:&lt;built-infunctionarray&gt;=None, params_all:&lt;built-\n                   infunctionarray&gt;=None,\n                   sepia_model:sepia.SepiaModel.SepiaModel=None,\n                   sepia_data:sepia.SepiaData.SepiaData=None,\n                   emulator_function=None, param_name:tuple=None,\n                   xy_lims:&lt;built-infunctionarray&gt;=[0.02, 10.0, 0.98,\n                   1.3])\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nk_all\narray\nNone\nall wavenumbers\n\n\nparams_all\narray\nNone\nall parameters\n\n\nsepia_model\nSepiaModel\nNone\nSEPIA emulator model\n\n\nsepia_data\nSepiaData\nNone\nInput data in SEPIA format\n\n\nemulator_function\nNoneType\nNone\nfunction which takes in sepia model and parameters\n\n\nparam_name\ntuple\nNone\nParameter name\n\n\nxy_lims\narray\n[0.02, 10.0, 0.98, 1.3]\n\n\n\n\n\nsource\n\n\nvalidation_plot\n\n validation_plot (k_all:&lt;built-infunctionarray&gt;=None, target_vals:&lt;built-\n                  infunctionarray&gt;=None, pred_mean:&lt;built-\n                  infunctionarray&gt;=None, pred_std:&lt;built-\n                  infunctionarray&gt;=None, xy_lims:&lt;built-\n                  infunctionarray&gt;=[0.02, 10.0, 0.98, 1.3])\n\n\nsource\n\n\nplot_mcmc\n\n plot_mcmc (samples:&lt;built-infunctionarray&gt;, params_list:list,\n            if_truth_know:bool=False)\n\n\n# #| hide\n\n# from getdist import plots, MCSamples\n# import re\n\n# def latex_to_plain(text):\n#     # Replace LaTeX specific symbols\n#     text = re.sub(r'\\$', '', text)  # Remove $ signs\n#     text = re.sub(r'\\\\', '', text)  # Remove backslashes\n#     text = re.sub(r'\\{|\\}', '', text)  # Remove braces\n#     return text\n\n\n# def plot_mcmc_getdist(samples:np.array, \n#               params_list:list, \n#               if_truth_know:bool=False):\n\n\n#     # from getdist.gaussian_mixtures import GaussianND\n#     # mat = - h\n#     # cov = np.linalg.pinv(mat)\n\n#     # covariance = cov #[[0.001**2, 0.0006*0.05, 0], [0.0006*0.05, 0.05**2, 0.2**2], [0, 0.2**2, 2**2]]\n#     # mean =np.array([para4[1], para5[1]]) #params #[0.02, 1, -2] \n#     # gauss=GaussianND(mean, covariance, names=['logfr0','n'], labels=[para4[0], para5[0]], label = 'Fisher')\n#     # # g = plots.get_subplot_plotter()\n#     # # g.triangle_plot(gauss,filled=True)\n\n#     # names = ['a', 'b', 'c', 'd', 'e']\n#     # s1 = samples_plot[samples_plot[:, 0] &gt; -5.3]\n\n#     s1 = samples\n#     param_names = [param[0] for param in params_list]\n#     PARAM_NAME_trial = np.array([latex_to_plain(item) for item in param_names]) #['Omega_m', 'n_s', '10^{9} A_s', 'h', 'f_\\phi']\n\n#     samples1 = MCSamples(samples=s1, \n#                         names=PARAM_NAME_trial, \n#                         labels=PARAM_NAME_trial, \n#                         label='MCMC' \n#                         )#, ranges={'logfr0':(-5.2, -4.8), 'n':(0.5, 1.5)})\n#     g = plots.get_subplot_plotter(subplot_size=4)\n#     g.settings.axes_fontsize=27\n#     g.settings.axes_labelsize = 27\n#     g.settings.legend_fontsize = 27\n#     g.settings.fontsize = 27\n#     g.settings.alpha_filled_add=0.6\n#     # g.settings.title_limit_fontsize = 27\n#     g.settings.solid_contour_palefactor = 0.5\n#     g.triangle_plot([samples1],  \n#                     # ['logfr0','n'], \n#                     filled=True, \n#                     # markers={'logfr0':para4[1], 'n':para5[1]}, \n#                     markers=[param[1] for param in params_list],\n#                     marker_args={'lw':2, 'ls':'dashed', 'color':'k'}\n#                     )\n\n#     # g.triangle_plot([samples1, gauss],  ['logfr0','n'], filled=True, markers={'logfr0':para4[1], 'n':para5[1]}, marker_args={'lw':2, 'ls':'dashed', 'color':'k'})\n#     # g.plot_contours(h, params, fill=True, alpha=0.5, label = 'Fisher info', facecolor = 'red')\n\n#     # g.export('../../../Plots/triangle_plot.png')\n\n\nsource\n\n\ngenerate_param_grid_with_fixed\n\n generate_param_grid_with_fixed (param_name:list=None,\n                                 param_indices:&lt;built-\n                                 infunctionarray&gt;=None,\n                                 fixed_params:&lt;built-\n                                 infunctionarray&gt;=None, param_min:&lt;built-\n                                 infunctionarray&gt;=None, param_max:&lt;built-\n                                 infunctionarray&gt;=None, steps:int=40)\n\n\nsource\n\n\nplot_error_heatmap\n\n plot_error_heatmap (errors:&lt;built-infunctionarray&gt;=None,\n                     param_names:list=None, param_range:tuple=None)",
    "crumbs": [
      "viz"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CubicGalileonEmu",
    "section": "",
    "text": "Modified Gravity emulator for boost in the dark matter power spectra",
    "crumbs": [
      "CubicGalileonEmu"
    ]
  },
  {
    "objectID": "index.html#install-for-development-not-production",
    "href": "index.html#install-for-development-not-production",
    "title": "CubicGalileonEmu",
    "section": "Install (for development, not production)",
    "text": "Install (for development, not production)\ngit clone https://github.com/nesar/CubicGalileonEmu.git\ncd CubicGalileonEmu/\npip install -e '.[dev]'",
    "crumbs": [
      "CubicGalileonEmu"
    ]
  },
  {
    "objectID": "index.html#basic-rundown",
    "href": "index.html#basic-rundown",
    "title": "CubicGalileonEmu",
    "section": "Basic rundown",
    "text": "Basic rundown\n\nA few imports\n\nfrom CubicGalileonEmu.load import *\nfrom CubicGalileonEmu.viz import *\nfrom CubicGalileonEmu.pca import *\nfrom CubicGalileonEmu.gp import *\nfrom CubicGalileonEmu.emu import *\nfrom CubicGalileonEmu.mcmc import *\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\n\n\nif_train_all = False ## Re-train all the models. Time-consuming. \nif_mcmc_all = False  ## Full MCMC run. Time-consuming. \nif_savefig = False\n\n\n\nLoading files\n\nBk_all, k_all, z_all = load_boost_data()\np_all = load_params()\n\n\n\nA few plotting routines\n\nExperimental design\n\ndf_train_a = pd.DataFrame(p_all, columns=PARAM_NAME)\ncolors = ['b']*p_all.shape[0]\n# colors = ['b']*num_sims + ['r']*num_sims_test\nplot_scatter_matrix(df_train_a, colors);\n\n\n\n\n\n\n\n\n\n\nBoost metrics colored by cosmology parameters\n\ncolor_by_index = 4\nz_index = 0\n\nplot_lines_with_param_color(p_all[:, color_by_index], \n                            k_all, \n                            Bk_all[:, z_index, :], \n                            'Training data, z=' + str(z_all[z_index]), \n                            r'$k [h/Mpc]$', \n                            r'$B(k)$', \n                            PARAM_NAME[color_by_index]);\n\n\n\n\n\n\n\n\n\n\n\nTraining involves: PCA, GP fitting.\n\n## Data prep\nz_index = 1\ny_vals = Bk_all[:, z_index, :]\n# y_ind = np.arange(0, y_vals.shape[1])\ny_ind = k_all\n\n# Train-test split\n# test_indices = [0, 14, 35]\n# input_params= p_all[test_indices]\n# target_vals = Bk_all[:, z_index, :][test_indices]\n\n# Load validation data\n\nBk_all_val, _, _ = load_boost_data(LIBRARY_BK_FILE_VAL, LIBRARY_ZK_FILE_VAL)\ntarget_vals = Bk_all_val[:, z_index, :]\ninput_params = load_params(LIBRARY_PARAM_FILE_VAL)\n\ntrain_indices = [i for i in  np.arange(49)] # if i not in test_indices]\np_all_train = p_all[train_indices]\ny_vals_train = Bk_all[:, z_index, :][train_indices]\nprint('Redshift: ' + str(z_all[z_index]))\n\nRedshift: 0.02\n\n\n\nsepia_data = sepia_data_format(p_all_train, y_vals_train, y_ind)\n# sepia_data = sepia_data_by_redshift(redshift=0.01)\n\nprint(sepia_data)\nmodel_filename = '../CubicGalileonEmu/model/multivariate_model_z_index' + str(z_index) \n\n# sepia_model = do_pca(sepia_data, exp_variance=0.95)\nsepia_model = do_pca(sepia_data, exp_variance=0.95)\n\nsepia_model = do_gp_train(sepia_model, model_filename)\nplot_train_diagnostics(sepia_model)\n\nThis SepiaData instance implies the following:\nThis is a simulator (eta)-only model, y dimension 768\nm  =    49 (number of simulated data)\np  =     1 (number of inputs)\nq  =     5 (number of additional simulation inputs)\npu NOT SET (transformed response dimension); call method create_K_basis \n\nStarting tune_step_sizes...\nDefault step sizes:\nbetaU\n[[0.1 0.1]\n [0.1 0.1]\n [0.1 0.1]\n [0.1 0.1]\n [0.1 0.1]\n [0.1 0.1]]\nlamUz\n[[5. 5.]]\nlamWs\n[[100. 100.]]\nlamWOs\n[[100.]]\n\n\nStep size tuning: 100%|██████████| 50/50 [00:07&lt;00:00,  6.30it/s]\n\n\nDone with tune_step_size.\nSelected step sizes:\nbetaU\n[[0.22281512 0.70746824]\n [0.06709009 0.25079165]\n [0.07520845 0.79598517]\n [0.07947789 0.59099223]\n [0.09855538 0.64676002]\n [0.79215786 0.60910325]]\nlamUz\n[[1.19332908 0.88828321]]\nlamWs\n[[ 391.11797876 2819.48252108]]\nlamWOs\n[[14.88176966]]\n\n\nMCMC sampling: 100%|██████████| 1000/1000 [00:07&lt;00:00, 134.33it/s]\n\n\nModel saved to ../CubicGalileonEmu/model/multivariate_model_z_index1.pkl\nNo thetas to plot\n\n\n\n\n\n\n\n\n\n\n\nLoad existing model\n\nsepia_model = gp_load(sepia_model, model_filename)\n\nWARNING: make sure this model was instantiated with the same input data as the model corresonding to this saved model info.\n\n\n\n\nSingle-redshift emulation for new cosmological parameters\n\ntest_indices_rand = np.random.randint(size=5, low=0, high=input_params.shape[0])\npred_mean, pred_std = emulate(sepia_model, sepia_data, input_params[test_indices_rand])\n# pred_quant == Emulated (0.05, 0.95) quantile\nf = validation_plot(k_all, target_vals[test_indices_rand], pred_mean, pred_std, xy_lims=[2e-2, 1e1, 0.98, 1.35]);\n\nf.savefig('/home/nramachandra/Projects/MG_emu/Plots/emu.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\nSensitivity analysis from the emulator\n\nf = sensitivity_plot(k_all, p_all, sepia_model, sepia_data, emulate, PARAM_NAME)\n\n\n\n\n\n\n\n\n\n\nMulti-redshift emulation\n\nTrain all the models\n\nif if_train_all:\n    \n    do_gp_train_multiple(model_dir='../CubicGalileonEmu/model/', \n                        p_train_all = p_all[train_indices],\n                        y_vals_all = Bk_all[train_indices],\n                        y_ind_all = k_all,\n                        z_index_range=range(49))\n\n\n\nLoad all trained models\n\nsepia_model_list, sepia_data_list = load_model_multiple(model_dir='../CubicGalileonEmu/model/', \n                                        p_train_all=p_all[train_indices],\n                                        y_vals_all=Bk_all[train_indices],\n                                        y_ind_all=k_all,\n                                        z_index_range=range(49), \n                                        sepia_model_i=sepia_model)\n\n\n\nEmulator confidence across parameter range\n\n# Parameter settings\nsteps = 50  # Number of steps in the grid for each parameter\nparam_name_extended = np.append(PARAM_NAME, 'Redshift')\nred_min = 0\nred_max = 3\nred_mean = 1.0\n\nparam_min = np.append(p_all.min(axis=0), red_min)\nparam_max = np.append(p_all.max(axis=0), red_max)\nparam_mean = np.append(p_all.mean(axis=0), red_mean)\n\n# Compute outputs and errors for a range of parameter values\ndef compute_errors(param_grid):\n    print(param_grid.shape)\n    errors = np.zeros(shape=(param_grid.shape[0], ))\n    # errors = np.array([np.mean(emu_redshift(params[np.newaxis, :], sepia_model_list, z_all)[1][:, 0, :]**2) for params in param_grid])\n    for par_indx in range(errors.shape[0]):\n        emu_pred = np.array(emu_redshift(param_grid[par_indx][np.newaxis, :], sepia_model_list, sepia_data_list, z_all))\n        errors_emu = emu_pred[1, :, 0]\n        print(errors_emu.shape)\n\n        errors[par_indx] = np.max(errors_emu)\n        print(errors_emu)\n\n    return errors.reshape(steps, steps)\n\n\n# Example usage:\nparam_indices = [4, 2]  # Indices of parameters to vary\nfixed_indices = [i for i in range(len(param_name_extended)) if i not in param_indices]\nfixed_params = {param_name_extended[i]: param_mean[i] for i in fixed_indices}\n\nparam_grid = generate_param_grid_with_fixed(param_name_extended, param_indices, fixed_params, param_min, param_max, steps)\n\nerrors = compute_errors(param_grid)\n\n\nf = plot_error_heatmap( errors, \n                       [param_name_extended[i] for i in param_indices], \n                       [(param_min[param_indices[0]], param_max[param_indices[0]]), (param_min[param_indices[1]], param_max[param_indices[1]])]\n                       )\n\nif if_savefig: \n    f.savefig('../../../Plots/heatmap_params_4_5.png', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\n\nParameter inference via MCMC using the emulator\n\nCreating mock observational data\n\ntarget_indx = 0 #0 ,14, 35\nz_index = 2\nL = 32\nfake_obs_data_index_every = 1\n\nredshift = z_all[z_index]\nx = k_all[::fake_obs_data_index_every]\ny = Bk_all[:, z_index, :][target_indx][::fake_obs_data_index_every]\ny = y + 1e-4*np.random.normal(0.0, 1.0, size=y.shape)\nyerr = np.sqrt( Bk_all[:, z_index, :][target_indx][::fake_obs_data_index_every]*(L**3))/(L**3)\n\nx_grid = k_all\nparams_calib = p_all[target_indx][:, np.newaxis].T\nprint('redshift: ', redshift)\n\n\nf, a = plt.subplots(1,1, figsize = (8, 5)) \ninput_params_and_redshift = np.append(params_calib, redshift)\nbk_target, err_target = emu_redshift(input_params_and_redshift[np.newaxis, :], sepia_model_list, sepia_data_list, z_all)\na.plot(k_all, bk_target[:, 0], label='Emulated at target params', lw=5, ls='--')\na.errorbar(x, y, yerr, label='Target mock observations', ls='none', lw=1, color = \"r\")\na.scatter(x, y, s = 5, marker = \"h\", color = \"r\")\n\n\na.plot(k_all, Bk_all[:, z_index, :].T, 'k', alpha=0.1)\n\n\nplt.plot(k_all, emulate(sepia_model_list[z_index], sepia_data, input_params_and_redshift[:-1])[0], label='grid z=%.4f'%z_all[z_index])\nplt.plot(k_all, emulate(sepia_model_list[z_index+1], sepia_data, input_params_and_redshift[:-1])[0], label='grid z=%.4f'%z_all[z_index + 1])\n\nstring_print0 = 'Target Params \\n\\n' \nstring_print1 = PARAM_NAME[0] + '= %.4f'%input_params_and_redshift[0] + '\\n'\nstring_print2 = PARAM_NAME[1] + '= %.4f'%input_params_and_redshift[1] + '\\n'\nstring_print3 = PARAM_NAME[2] + '= %.4f'%input_params_and_redshift[2] + '\\n'\nstring_print4 = PARAM_NAME[3] + '= %.4f'%input_params_and_redshift[3] + '\\n'\nstring_print5 = PARAM_NAME[4] + '= %.4f'%input_params_and_redshift[4] + '\\n'\nstring_print6 = 'redshift' + '= %.4f'%input_params_and_redshift[5] \n\n\nstring_print = string_print0 + string_print1 + string_print2 + string_print3 + string_print4 + string_print5 + string_print6\n\nprops = dict(boxstyle='round', facecolor='gray', alpha=0.2)\nplt.text(1.02, 0.1, string_print, transform=a.transAxes, fontsize=12, bbox=props)\n\n\na.set_xscale('log')\nplt.title('pre-MCMC')\na.set_xlabel(r'$k [h/Mpc]$')\na.set_ylabel(r'$B(k)$')\nplt.legend()\n\n\n\n\n\n\n\n\n\npos0 = chain_init(params_list, ndim, nwalkers)\nsampler = define_sampler(redshift, ndim, nwalkers, params_list, x_grid, sepia_model_list, sepia_data_list, z_all, x, y, yerr)\n\n\nMCMC run - first burn, then full.\n\npos, prob, state, samples, sampler, autocorr, index = do_mcmc(sampler, pos0, nrun_burn, ndim, if_burn=True)\n\nif if_mcmc_all: # Full MCMC-run, will be slow\n    pos, prob, state, samples, sampler, autocorr, index = do_mcmc(sampler, pos, nrun, ndim, if_burn=False)\n\np_mcmc = mcmc_results(samples)\n\nfig = plot_mcmc(samples, params_list, if_truth_know=True)\nif if_savefig: \n    plt.savefig('../../../Plots/mcmc_plot.png', bbox_inches='tight')\n\n100%|██████████| 100/100 [00:23&lt;00:00,  4.34it/s]\n\n\n\n\n\n\n\n\n\n\nf, a = plt.subplots(1,1, figsize = (8, 5)) \ninput_params_and_redshift = np.append(p_mcmc, redshift)\nbk_mcmc, err_mcmc = emu_redshift(input_params_and_redshift[np.newaxis, :], sepia_model_list, sepia_data_list, z_all)\na.plot(k_all, bk_mcmc[:, 0], label='Emulated at best MCMC', lw=3, ls='--')\na.errorbar(x, y, yerr, label='Mock target', ls='none', lw=1, color = \"r\")\na.scatter(x, y, s = 5, marker = \"h\", color = \"r\", alpha=0.5)\n\na.plot(k_all, Bk_all[:, z_index, :].T, 'k', alpha=0.1)\n\n\n# plt.plot(k_all, emulate(sepia_model_list[z_index], input_params_and_redshift[:-1])[0], label='Z1')\n# plt.plot(k_all, emulate(sepia_model_list[z_index+1], input_params_and_redshift[:-1])[0], label='Z2')\n\nstring_print0 = 'Target Params \\n\\n' \nstring_print1 = PARAM_NAME[0] + '= %.3f'%params_calib[0][0] + '\\n'\nstring_print2 = PARAM_NAME[1] + '= %.3f'%params_calib[0][1] + '\\n'\nstring_print3 = PARAM_NAME[2] + '= %.3f'%params_calib[0][2] + '\\n'\nstring_print4 = PARAM_NAME[3] + '= %.3f'%params_calib[0][3] + '\\n'\nstring_print5 = PARAM_NAME[4] + '= %.3f'%params_calib[0][4] + '\\n'\nstring_print6 = 'redshift' + '= %.3f'%redshift\n\n\nstring_print = string_print0 + string_print1 + string_print2 + string_print3 + string_print4 + string_print5 + string_print6\n\nprops = dict(boxstyle='round', facecolor='gray', alpha=0.2)\nplt.text(1.02, 0.5, string_print, transform=a.transAxes, fontsize=12, bbox=props)\n\nstring_print0_mcmc = 'Optimized Params \\n\\n' \nstring_print1_mcmc = PARAM_NAME[0] + '= %.3f'%p_mcmc[0] + '\\n'\nstring_print2_mcmc = PARAM_NAME[1] + '= %.3f'%p_mcmc[1] + '\\n'\nstring_print3_mcmc = PARAM_NAME[2] + '= %.3f'%p_mcmc[2] + '\\n'\nstring_print4_mcmc = PARAM_NAME[3] + '= %.3f'%p_mcmc[3] + '\\n'\nstring_print5_mcmc = PARAM_NAME[4] + '= %.3f'%p_mcmc[4] \n\nstring_print_mcmc = string_print0_mcmc + string_print1_mcmc + string_print2_mcmc + string_print3_mcmc + string_print4_mcmc + string_print5_mcmc\n\nprops = dict(boxstyle='round', facecolor='blue', alpha=0.2)\nplt.text(1.02, 0.05, string_print_mcmc, transform=a.transAxes, fontsize=12, bbox=props)\n\n\n\na.set_xscale('log')\nplt.title('B(k) at MCMC constraints')\na.set_xlabel(r'$k [h/Mpc]$')\na.set_ylabel(r'$B(k)$')\nplt.legend()\n\nif if_savefig: \n    plt.savefig('../../../Plots/mcmc_results_Bk.png', bbox_inches='tight')",
    "crumbs": [
      "CubicGalileonEmu"
    ]
  },
  {
    "objectID": "mcmc.html",
    "href": "mcmc.html",
    "title": "mcmc",
    "section": "",
    "text": "source\n\nln_prior\n\n ln_prior (theta, params_list)\n\n\nsource\n\n\nln_like\n\n ln_like (theta, redshift, x_grid, sepia_model_list, sepia_data_list,\n          z_all, x, y, yerr)\n\n\nsource\n\n\nln_prob\n\n ln_prob (theta, redshift, params_list, x_grid, sepia_model_list,\n          sepia_data_list, z_all, x, y, yerr)\n\n\nsource\n\n\nchain_init\n\n chain_init (params_list, ndim, nwalkers)\n\n\nsource\n\n\ndefine_sampler\n\n define_sampler (redshift, ndim, nwalkers, params_list, x_grid,\n                 sepia_model_list, sepia_data_list, z_all, x, y, yerr)\n\n\nsource\n\n\ndo_mcmc\n\n do_mcmc (sampler, pos, nrun, ndim, if_burn=False)\n\n\nsource\n\n\nmcmc_results\n\n mcmc_results (samples)",
    "crumbs": [
      "mcmc"
    ]
  }
]